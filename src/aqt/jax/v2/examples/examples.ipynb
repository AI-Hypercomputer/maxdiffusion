{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTi2Qa2Q6I6Q"
      },
      "source": [
        "# AQT Tutorial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0o0SRAQF3Ub"
      },
      "outputs": [],
      "source": [
        "# install the AQT library\n",
        "!pip install aqtp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9-GOGZSZ8RC"
      },
      "outputs": [],
      "source": [
        "# necessary imports\n",
        "import aqt.jax.v2.flax.aqt_flax as aqt\n",
        "import aqt.jax.v2.config as aqt_config\n",
        "import flax.linen as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fh3igA7UHU1K"
      },
      "outputs": [],
      "source": [
        "class MlpBlock(nn.Module):\n",
        "  config: aqt_config.DotGeneral | None\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, inputs):\n",
        "    dot_general = aqt.AqtDotGeneral(self.config)\n",
        "    x = nn.Dense(dot_general=dot_general, features=inputs.shape[-1] * 4)(inputs)\n",
        "    x = nn.relu(x)\n",
        "    x = nn.Dense(dot_general=dot_general, features=inputs.shape[-1])(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-athz_HHkVC"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "\n",
        "# Generate some random matrices as inputs\n",
        "def gen_matrix(rows, columns, seed=0):\n",
        "  np.random.seed(seed)\n",
        "  return np.random.normal(size=(rows, columns)).reshape((rows, columns))\n",
        "\n",
        "inputs = gen_matrix(3, 4)\n",
        "\n",
        "# test function that initializes the model and compute the forward pass\n",
        "def init_and_eval(name, mlp_block, init_seed=0, eval_seed=0):\n",
        "  model = mlp_block.init(jax.random.PRNGKey(init_seed), inputs)\n",
        "  out = mlp_block.apply(model, inputs, rngs={'params': jax.random.key(eval_seed)})\n",
        "  print(f\"{name}:\\n\", out)\n",
        "\n",
        "# create a config that quantizes both forward and backward passes to int8\n",
        "int8_config = aqt_config.fully_quantized(fwd_bits=8, bwd_bits=8)\n",
        "\n",
        "# run and print results\n",
        "mlp_fp16 = MlpBlock(config=None)\n",
        "mlp_int8 = MlpBlock(config=int8_config)\n",
        "init_and_eval('mlp_fp16', mlp_fp16)\n",
        "init_and_eval('mlp_int8', mlp_int8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNW6Zpab6MRM"
      },
      "source": [
        "# How AQT Works Internally"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDofS2xm6QpT"
      },
      "outputs": [],
      "source": [
        "import jax.numpy as jnp\n",
        "\n",
        "def matmul_true_int8(lhs, rhs):\n",
        "  assert lhs.dtype == jnp.int8\n",
        "  assert rhs.dtype == jnp.int8\n",
        "  result = jnp.matmul(lhs, rhs, preferred_element_type=jnp.int32)\n",
        "  assert result.dtype == jnp.int32\n",
        "  return result\n",
        "\n",
        "# Generate some random matrices as inputs\n",
        "def gen_matrix(rows, columns, seed=0):\n",
        "  import numpy as np\n",
        "  np.random.seed(seed)\n",
        "  return np.random.normal(size=(rows, columns)).reshape((rows, columns))\n",
        "\n",
        "batch_size = 3\n",
        "channels_in = 4\n",
        "channels_out = 5\n",
        "a = gen_matrix(batch_size, channels_in) # Activations\n",
        "w = gen_matrix(channels_in, channels_out) # Weights\n",
        "\n",
        "def aqt_matmul_int8(a, w):\n",
        "  max_int8 = 127\n",
        "  # This function is customizable and injectable, i.e:\n",
        "  # users can inject custom quant code into an AQT config.\n",
        "  def quant_int8(x):\n",
        "    return jnp.clip(jnp.round(x), -max_int8, max_int8).astype(jnp.int8)\n",
        "\n",
        "  # Calibration. Calibration function is also customizable and injectable.\n",
        "  a_s = max_int8 / jnp.max(jnp.abs(a), axis=1, keepdims=True)\n",
        "  w_s = max_int8 / jnp.max(jnp.abs(w), axis=0, keepdims=True)\n",
        "  assert a_s.shape == (batch_size, 1) # shapes checked for illustration\n",
        "  assert w_s.shape == (1, channels_out)\n",
        "\n",
        "  # int8 matmul with int32 accumulator\n",
        "  result = matmul_true_int8(quant_int8(a * a_s), quant_int8(w * w_s)) / (a_s * w_s)\n",
        "  assert result.shape == (batch_size, channels_out)\n",
        "\n",
        "  return result\n",
        "\n",
        "# Test\n",
        "print(f\"jnp.matmul(a, w):\\n\", jnp.matmul(a, w))\n",
        "print(f\"aqt_matmul_int8(a, w):\\n\", aqt_matmul_int8(a, w))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1AF_HtCk-VhTBUQ1JdeIY0NeXAWjdICMf",
          "timestamp": 1698963395477
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
