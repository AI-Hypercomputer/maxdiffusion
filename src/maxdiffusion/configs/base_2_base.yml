# Copyright 2023 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# This sentinel is a reminder to choose a real run name.
run_name: ''
metrics_file: "" # for testing, local file that stores scalar metrics. If empty, no metrics are written.
# If true save metrics such as loss and TFLOPS to GCS in {base_output_directory}/{run_name}/metrics/
write_metrics: False # write metrics with a buffer
gcs_metrics: False
metrics_period: 100  # calculate step time and loss
# If true save config to GCS in {base_output_directory}/{run_name}/
save_config_to_gcs: False
log_period: 10000000000  # Flushes Tensorboard

pretrained_model_name_or_path: 'gs://jfacevedo-maxdiffusion-v5p/stable_diffusion_checkpoints/models--stabilityai--stable-diffusion-2-base'
unet_checkpoint: ''
revision: 'main'
dtype: 'bfloat16'
# Set true to load weights from pytorch
from_pt: False
split_head_dim: True
attention: 'flash' # Supported attention: dot_product, flash
flash_block_sizes: {}
# to override default block sizes for flash attention
# flash_block_sizes:
#   block_q: 64
#   block_kv_compute: 64
#   block_kv: 64
#   block_q_dkv: 64
#   block_kv_dkv: 64
#   block_kv_dkv_compute: 64
#   block_q_dq: 64
#   block_kv_dq: 64

# GroupNorm groups
norm_num_groups: 32

# If train_new_unet, unet weights will be randomly initialized to train the unet from scratch
# else they will be loaded from pretrained_model_name_or_path
train_new_unet: False

# https://www.crosslabs.org/blog/diffusion-with-offset-noise
noise_offset: -1.0 #0.05
# https://arxiv.org/pdf/2301.11706.pdf
input_peturbation: -1.0 #0.1
# https://arxiv.org/pdf/2305.08891.pdf
snr_gamma: -1.0
# values are v_prediction or leave empty to use scheduler's default.
prediction_type: 'v_prediction'

# Output directory
# Create a GCS bucket, e.g. my-maxtext-outputs and set this to "gs://my-maxtext-outputs/"
base_output_directory: ""

# Parallelism
mesh_axes: ['data', 'fsdp', 'tensor']

# batch : batch dimension of data and activations
# hidden : 
# embed : attention qkv dense layer hidden dim named as embed
# heads : attention head dim = num_heads * head_dim
# length : attention sequence length
# temb_in : dense.shape[0] of resnet dense before conv 
# out_c : dense.shape[1] of resnet dense before conv 
# out_channels : conv.shape[-1] activation
# keep_1 : conv.shape[0] weight
# keep_2 : conv.shape[1] weight
# conv_in : conv.shape[2] weight
# conv_out : conv.shape[-1] weight
logical_axis_rules: [
                      ['batch', 'data'],
                      ['activation_batch', ['data','fsdp']],
                      ['activation_heads', 'tensor'],
                      ['activation_kv', 'tensor'],
                      ['embed', 'fsdp'],
                      ['heads', 'tensor'],
                      ['conv_batch', ['data','fsdp']],
                      ['out_channels', 'tensor'],
                      ['conv_out', 'fsdp'],
                    ]
data_sharding: [['data', 'fsdp', 'tensor']]

# One axis for each parallelism type may hold a placeholder (-1)
# value to auto-shard based on available slices and devices.
# By default, product of the DCN axes should equal number of slices
# and product of the ICI axes should equal number of devices per slice.
dcn_data_parallelism: -1  # recommended DCN axis to be auto-sharded
dcn_fsdp_parallelism: 1
dcn_tensor_parallelism: 1
ici_data_parallelism: -1 # recommended ICI axis to be auto-sharded for TPUv5e 
ici_fsdp_parallelism: 1  # recommended ICI axis to be auto-sharded
ici_tensor_parallelism: 1

# Dataset
# Replace with dataset path or train_data_dir. One has to be set.
dataset_name: ''
train_data_dir: 'gs://jfacevedo-maxdiffusion/laion400m/tf_records'
dataset_config_name: ''
cache_dir: ''
image_column: 'image'
caption_column: 'text'
resolution: 512
center_crop: False
random_flip: False
# If cache_latents_text_encoder_outputs is True
# the num_proc is set to 1
tokenize_captions_num_proc: 4
transform_images_num_proc: 4
reuse_example_batch: False
enable_data_shuffling: True
# checkpoint every number of samples
checkpoint_every: 512000
start_step_to_checkpoint: 512000
eval_at_checkpoint: False
upload_ckpts_to_gcs: False
upload_images: False
cache_latents_text_encoder_outputs: False

# Evaluation dataset
dataset_name: ''
eval_data_dir_captions: ''
eval_data_dir_stats: ''

# Training loop
learning_rate: 1.25e-7
scale_lr: False
max_train_samples: -1
max_train_steps: 25442
seed: 0
output_dir: 'sd-model-finetuned'
per_device_batch_size: 4
pre_compile: true

warmup_steps_fraction: 0.33
learning_rate_schedule_steps: -1 # By default the length of the schedule is set to the number of steps.
learning_rate_init_value: 1.e-6 # initial value for learning rate schedule
learning_rate_scheduler: "cosine" # schedulers: cosine or linear
# schedulers: ddim, ddpm
training_scheduler: "ddpm"

# However you may choose a longer schedule (learning_rate_schedule_steps > steps), in which case the training will end before 
# dropping fully down. Or you may choose a shorter schedule, where the unspecified steps will have a learning rate of 0.

# AdamW optimizer parameters
adam_b1: 0.9 # Exponential decay rate to track the first moment of past gradients.
adam_b2: 0.999 # Exponential decay rate to track the second moment of past gradients.
adam_eps: 1.e-8 # A small constant applied to denominator outside of the square root.
adam_weight_decay: 1.e-2 # AdamW Weight decay
max_grad_norm: 1.0

enable_profiler: False
# Skip first n steps for profiling, to omit things like compilation and to give
# the iteration time a chance to stabilize.
skip_first_n_steps_for_profiler: 1
profiler_steps: 5

# Generation parameters
prompts: "/home/shahrokhi/prompts.txt"
image_ids: "/home/shahrokhi/ids.txt"
images_directory: "/home/shahrokhi/maxdiffusion/generated_images/"
stat_output_directory: "/home/shahrokhi/maxdiffusion/output/"
stat_output_file: "/home/shahrokhi/maxdiffusion/output/stats.npz"
stat_coco_file: "/home/shahrokhi/coco2014/val2014_30k_stats.npz"
caption_coco_file: "/home/shahrokhi/coco2014/val2014_30k.tsv"
clip_cache_dir: "/home/shahrokhi/maxdiffusion/clip_cache_dir"
negative_prompt: ""
# https://github.com/mlcommons/training/blob/master/stable_diffusion/configs/train_01x08x08.yaml#L26
guidance_scale: 8
num_inference_steps: 50
rescale_zero_terminal_snr: True
# trailing or leading - set trailing if using rescale_zero_terminal_snr
timestep_spacing: "trailing"
# schedulers : ddpm, ddim
inference_scheduler: "ddim"
# For preprocessing data to tfrecords
data_files_pattern: ""
extracted_files_dir: ""
tfrecords_dir: ""
no_records_per_shard: 1000

inception_weights_path: 'gs://jfacevedo-maxdiffusion-v5p/inception_checkpoints/inception_v3/inception_v3_weights_fid.pickle'
clip_model_name_or_path: 'gs://jfacevedo-maxdiffusion-v5p/CLIP_checkpoints/models--laion--CLIP-ViT-H-14-laion2B-s32B-b79K'

enable_mllog: True
