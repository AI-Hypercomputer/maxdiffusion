# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# DISCLAIMER: reference pytorch implementation: https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_unipc_multistep.py

from typing import List, Optional, Tuple, Union

import flax
import jax
import jax.numpy as jnp
from functools import partial

from ..configuration_utils import ConfigMixin, register_to_config
from ..utils import is_scipy_available
from .scheduling_utils_flax import (
    CommonSchedulerState,
    FlaxSchedulerMixin,
    FlaxSchedulerOutput,
    add_noise_common,
)

def check_nan_jit(tensor: jax.Array, name: str, step: jax.Array):
    if tensor is None:
      return
    has_nans = jnp.isnan(tensor).any()
    has_infs = jnp.isinf(tensor).any()
    if step is None:
        step = -1

    # Print the actual dtype of the tensor's data
    jax.debug.print(f"[DEBUG SCHEDULER {jax.process_index()}] Step: {{step}} - {name}: "
                    "Shape: {shape}, tensor.dtype: {dtype}, Has NaNs: {has_nans_val}, Has Infs: {has_infs_val}",
                    step=step, shape=tensor.shape, dtype=tensor.dtype, has_nans_val=has_nans, has_infs_val=has_infs)

@flax.struct.dataclass
class UniPCMultistepSchedulerState:
  """
  Data class to hold the mutable state of the FlaxUniPCMultistepScheduler.
  """

  common: CommonSchedulerState

  # Core schedule parameters (derived from CommonSchedulerState in create_state)
  sigmas: jnp.ndarray
  alpha_t: jnp.ndarray
  sigma_t: jnp.ndarray
  lambda_t: jnp.ndarray
  init_noise_sigma: float

  # History buffers for multi-step solver
  # `model_outputs` stores previous converted model outputs (e.g., predicted x0 or epsilon)
  timesteps: jnp.ndarray = None
  model_outputs: jnp.ndarray = None
  timestep_list: jnp.ndarray = None  # Stores corresponding timesteps for `model_outputs`

  # State variables for tracking progress and solver order
  lower_order_nums: int = 0
  last_sample: Optional[jnp.ndarray] = None  # Sample from the previous predictor step
  step_index: Optional[int] = None
  begin_index: Optional[int] = None  # Used for img2img/inpaing
  this_order: int = 0  # Current effective order of the UniPC solver for this step

  @classmethod
  def create(
      cls,
      common_state: CommonSchedulerState,
      alpha_t: jnp.ndarray,
      sigma_t: jnp.ndarray,
      lambda_t: jnp.ndarray,
      sigmas: jnp.ndarray,
      init_noise_sigma: jnp.ndarray,
  ):
    return cls(
        common=common_state,
        alpha_t=alpha_t,
        sigma_t=sigma_t,
        lambda_t=lambda_t,
        sigmas=sigmas,
        init_noise_sigma=init_noise_sigma,
        lower_order_nums=0,
        last_sample=None,
        step_index=None,
        begin_index=None,
        this_order=0,
    )


@flax.struct.dataclass(frozen=False)
class FlaxUniPCMultistepSchedulerOutput(FlaxSchedulerOutput):
  state: UniPCMultistepSchedulerState


class FlaxUniPCMultistepScheduler(FlaxSchedulerMixin, ConfigMixin):
  """
  `FlaxUniPCMultistepScheduler` is a JAX/Flax training-free framework designed for the fast sampling of diffusion models.
  It implements the UniPC (Unified Predictor-Corrector) algorithm for efficient diffusion model sampling.
  """

  dtype: jnp.dtype

  @property
  def has_state(self) -> bool:
    return True

  @register_to_config
  def __init__(
      self,
      num_train_timesteps: int = 1000,
      beta_start: float = 0.0001,
      beta_end: float = 0.02,
      beta_schedule: str = "linear",
      trained_betas: Optional[Union[jnp.ndarray, List[float]]] = None,
      solver_order: int = 2,
      prediction_type: str = "epsilon",
      thresholding: bool = False,
      dynamic_thresholding_ratio: float = 0.995,
      sample_max_value: float = 1.0,
      predict_x0: bool = True,
      solver_type: str = "bh2",
      lower_order_final: bool = True,
      disable_corrector: List[int] = [],
      solver_p: Optional[FlaxSchedulerMixin] = None,
      use_karras_sigmas: Optional[bool] = False,
      use_exponential_sigmas: Optional[bool] = False,
      use_beta_sigmas: Optional[bool] = False,
      use_flow_sigmas: Optional[bool] = False,
      flow_shift: Optional[float] = 1.0,
      timestep_spacing: str = "linspace",
      steps_offset: int = 0,
      final_sigmas_type: Optional[str] = "zero",
      rescale_zero_terminal_snr: bool = False,
      dtype: jnp.dtype = jnp.float32,
  ):
    self.dtype = dtype

    # Validation checks from original __init__
    if self.config.use_beta_sigmas and not is_scipy_available():
      raise ImportError("Make sure to install scipy if you want to use beta sigmas.")
    if (
        sum(
            [
                self.config.use_beta_sigmas,
                self.config.use_exponential_sigmas,
                self.config.use_karras_sigmas,
            ]
        )
        > 1
    ):
      raise ValueError(
          "Only one of `config.use_beta_sigmas`, `config.use_exponential_sigmas`, `config.use_karras_sigmas` can be used."
      )
    if self.config.solver_type not in ["bh1", "bh2"]:
      raise NotImplementedError(f"{self.config.solver_type} is not implemented for {self.__class__}")

  def create_state(self, common: Optional[CommonSchedulerState] = None) -> UniPCMultistepSchedulerState:
    if common is None:
      common = CommonSchedulerState.create(self)

    if self.config.get("rescale_zero_terminal_snr", False):
      # Close to 0 without being 0 so first sigma is not inf
      # FP16 smallest positive subnormal works well here
      alphas_cumprod = common.alphas_cumprod
      alphas_cumprod = alphas_cumprod.at[-1].set(2**-24)
      common = common.replace(alphas_cumprod=alphas_cumprod)

    # Currently we only support VP-type noise schedule
    alpha_t = jnp.sqrt(common.alphas_cumprod)
    sigma_t = jnp.sqrt(1 - common.alphas_cumprod)
    lambda_t = jnp.log(alpha_t) - jnp.log(sigma_t)
    sigmas = ((1 - common.alphas_cumprod) / common.alphas_cumprod) ** 0.5

    # standard deviation of the initial noise distribution
    init_noise_sigma = jnp.array(1.0, dtype=self.dtype)

    if self.config.solver_type not in ["bh1", "bh2"]:
      if self.config.solver_type in ["midpoint", "heun", "logrho"]:
        self.config.solver_type = "bh2"
      else:
        raise NotImplementedError(f"{self.config.solver_type} is not implemented for {self.__class__}")

    return UniPCMultistepSchedulerState.create(
        common_state=common,
        alpha_t=alpha_t,
        sigma_t=sigma_t,
        lambda_t=lambda_t,
        sigmas=sigmas,
        init_noise_sigma=init_noise_sigma,
    )

  def set_begin_index(self, state: UniPCMultistepSchedulerState, begin_index: int = 0) -> UniPCMultistepSchedulerState:
    """
    Sets the begin index for the scheduler. This function should be run from pipeline before the inference.
    """
    return state.replace(begin_index=begin_index)

  def set_timesteps(
      self,
      state: UniPCMultistepSchedulerState,
      num_inference_steps: int,
      shape: Tuple,
  ) -> UniPCMultistepSchedulerState:
    """
    Sets the discrete timesteps used for the diffusion chain (to be run before inference).
    """
    #### Copied from scheduling_dmpsolver_multistep_flax
    last_timestep = self.config.num_train_timesteps
    if self.config.timestep_spacing == "linspace":
      timesteps = jnp.linspace(0, last_timestep - 1, num_inference_steps + 1).round()[::-1][:-1].astype(jnp.int32)
    elif self.config.timestep_spacing == "leading":
      step_ratio = last_timestep // (num_inference_steps + 1)
      # creates integer timesteps by multiplying by ratio
      # casting to int to avoid issues when num_inference_step is power of 3
      timesteps = (jnp.arange(0, num_inference_steps + 1) * step_ratio).round()[::-1][:-1].copy().astype(jnp.int32)
      timesteps += self.config.steps_offset
    elif self.config.timestep_spacing == "trailing":
      step_ratio = self.config.num_train_timesteps / num_inference_steps
      # creates integer timesteps by multiplying by ratio
      # casting to int to avoid issues when num_inference_step is power of 3
      timesteps = jnp.arange(last_timestep, 0, -step_ratio).round().copy().astype(jnp.int32)
      timesteps -= 1
    else:
      raise ValueError(
          f"{self.config.timestep_spacing} is not supported. Please make sure to choose one of 'linspace', 'leading' or 'trailing'."
      )

    # initial running values
    sigmas = state.sigmas

    # TODO
    # # Apply Karras/Exponential/Beta/Flow Sigmas if configured
    if self.config.use_karras_sigmas:
      # sigmas = _convert_to_karras_jax(in_sigmas=sigmas, num_inference_steps=num_inference_steps)
      # timesteps = jnp.array([_sigma_to_t_jax(s, log_sigmas_full) for s in sigmas]).round().astype(jnp.int64)
      raise NotImplementedError("`use_karras_sigmas` is not implemented in JAX version yet.")
    elif self.config.use_exponential_sigmas:
      # sigmas = _convert_to_exponential_jax(in_sigmas=sigmas, num_inference_steps=num_inference_steps)
      # timesteps = jnp.array([_sigma_to_t_jax(s, log_sigmas_full) for s in sigmas]).round().astype(jnp.int64)
      raise NotImplementedError("`use_exponential_sigmas` is not implemented in JAX version yet.")
    elif self.config.use_beta_sigmas:
      # sigmas = _convert_to_beta_jax(in_sigmas=sigmas, num_inference_steps=num_inference_steps)
      # timesteps = jnp.array([_sigma_to_t_jax(s, log_sigmas_full) for s in sigmas]).round().astype(jnp.int64)
      raise NotImplementedError("`use_beta_sigmas` is not implemented in JAX version yet.")
    if self.config.use_flow_sigmas:
      alphas = jnp.linspace(1, 1 / self.config.num_train_timesteps, num_inference_steps + 1)
      sigmas = 1.0 - alphas
      sigmas = jnp.flip(self.config.flow_shift * sigmas / (1 + (self.config.flow_shift - 1) * sigmas))[:-1].copy()
      timesteps = (sigmas * self.config.num_train_timesteps).copy().astype(jnp.int64)
      if self.config.final_sigmas_type == "sigma_min":
        sigma_last = sigmas[-1]
      elif self.config.final_sigmas_type == "zero":
        sigma_last = 0
      else:
        raise ValueError(
            f"`final_sigmas_type` must be one of 'zero', or 'sigma_min', but got {self.config.final_sigmas_type}"
        )
      sigmas = jnp.concatenate([sigmas, jnp.array([sigma_last])]).astype(jnp.float32)
    else:  # Default case if none of the specialized sigmas are used
      sigmas = jnp.interp(timesteps, jnp.arange(0, len(sigmas)), sigmas)
      if self.config.final_sigmas_type == "sigma_min":
        sigma_last = ((1 - state.common.alphas_cumprod[0]) / state.common.alphas_cumprod[0]) ** 0.5
      elif self.config.final_sigmas_type == "zero":
        sigma_last = 0
      else:
        raise ValueError(
            f"`final_sigmas_type` must be one of 'zero', or 'sigma_min', but got {self.config.final_sigmas_type}"
        )
      sigmas = jnp.concatenate([sigmas, jnp.array([sigma_last])]).astype(jnp.float32)

    model_outputs = jnp.zeros((self.config.solver_order,) + shape, dtype=self.dtype)
    timestep_list = jnp.zeros((self.config.solver_order,), dtype=jnp.int32)  # Timesteps are integers
    # Update the state with the new schedule and re-initialized history
    return state.replace(
        timesteps=timesteps,
        sigmas=sigmas,
        model_outputs=model_outputs,
        timestep_list=timestep_list,
        lower_order_nums=0,  # Reset counters for a new inference run
        step_index=None,
        begin_index=None,
        last_sample=None,
        this_order=0,
    )

  def convert_model_output(
      self,
      state: UniPCMultistepSchedulerState,
      model_output: jnp.ndarray,
      sample: jnp.ndarray,
      step: jax.Array,
  ) -> jnp.ndarray:
    """
    Converts the model output based on the prediction type and current state.
    """
    sigma = state.sigmas[state.step_index]  # Current sigma
    check_nan_jit(sigma, "convert_model_output sigma", step)

    # Ensure sigma is a JAX array for _sigma_to_alpha_sigma_t
    alpha_t, sigma_t = self._sigma_to_alpha_sigma_t(sigma)
    check_nan_jit(alpha_t, "convert_model_output alpha_t", step)
    check_nan_jit(sigma_t, "convert_model_output sigma_t", step)

    if self.config.predict_x0:
      if self.config.prediction_type == "epsilon":
        x0_pred = (sample - sigma_t * model_output) / alpha_t
      elif self.config.prediction_type == "sample":
        x0_pred = model_output
      elif self.config.prediction_type == "v_prediction":
        x0_pred = alpha_t * sample - sigma_t * model_output
      elif self.config.prediction_type == "flow_prediction":
        # Original code has `sigma_t = self.sigmas[self.step_index]`.
        # This implies current sigma `sigma` is used as sigma_t for flow.
        x0_pred = sample - sigma * model_output
      else:
        raise ValueError(
            f"prediction_type given as {self.config.prediction_type} must be one of `epsilon`, `sample`, "
            "`v_prediction`, or `flow_prediction` for the UniPCMultistepScheduler."
        )
      check_nan_jit(x0_pred, "convert_model_output x0_pred", step)

      if self.config.thresholding:
        raise NotImplementedError("Dynamic thresholding isn't implemented.")
        # x0_pred = self._threshold_sample(x0_pred)
      return x0_pred
    else:  # self.config.predict_x0 is False
      if self.config.prediction_type == "epsilon":
        return model_output
      elif self.config.prediction_type == "sample":
        epsilon = (sample - alpha_t * model_output) / sigma_t
        return epsilon
      elif self.config.prediction_type == "v_prediction":
        epsilon = alpha_t * model_output + sigma_t * sample
        return epsilon
      else:
        raise ValueError(
            f"prediction_type given as {self.config.prediction_type} must be one of `epsilon`, `sample`, or"
            " `v_prediction` for the UniPCMultistepScheduler."
        )

  def multistep_uni_p_bh_update(
      self,
      state: UniPCMultistepSchedulerState,
      model_output: jnp.ndarray,
      sample: jnp.ndarray,
      order: int,
      step: jax.Array,
  ) -> jnp.ndarray:
    """
    One step for the UniP (B(h) version) - the Predictor.
    """
    if self.config.solver_p:
      raise NotImplementedError("Nested `solver_p` is not implemented in JAX version yet.")

    m0 = state.model_outputs[self.config.solver_order - 1]  # Most recent stored converted model output
    check_nan_jit(m0, "P m0", step)
    x = sample
    check_nan_jit(x, "P sample", step)

    sigma_t_val, sigma_s0_val = (
        state.sigmas[state.step_index + 1],
        state.sigmas[state.step_index],
    )
    check_nan_jit(sigma_t_val, "P sigma_t_val", step)
    check_nan_jit(sigma_s0_val, "P sigma_s0_val", step)


    alpha_t, sigma_t = self._sigma_to_alpha_sigma_t(sigma_t_val)
    check_nan_jit(alpha_t, "P alpha_t", step)
    check_nan_jit(sigma_t, "P sigma_t", step)

    alpha_s0, sigma_s0 = self._sigma_to_alpha_sigma_t(sigma_s0_val)
    check_nan_jit(alpha_s0, "P alpha_s0", step)
    check_nan_jit(sigma_s0, "P sigma_s0", step)

    lambda_t = jnp.log(alpha_t + 1e-5) - jnp.log(sigma_t + 1e-5)
    check_nan_jit(lambda_t, "P lambda_t", step)
    lambda_s0 = jnp.log(alpha_s0 + 1e-5) - jnp.log(sigma_s0 + 1e-5)
    check_nan_jit(lambda_s0, "P lambda_s0", step)

    h = lambda_t - lambda_s0
    check_nan_jit(h, "P h", step)

    def rk_d1_loop_body(i, carry):
      # Loop from i = 0 to order-2
      rks, D1s = carry
      history_idx = self.config.solver_order - 2 - i
      mi = state.model_outputs[history_idx]
      check_nan_jit(mi, f"P rk_d1 mi[{i}]", step)
      si_val = state.timestep_list[history_idx]

      alpha_si, sigma_si = self._sigma_to_alpha_sigma_t(state.sigmas[self.index_for_timestep(state, si_val)])
      check_nan_jit(alpha_si, f"P rk_d1 alpha_si[{i}]", step)
      check_nan_jit(sigma_si, f"P rk_d1 sigma_si[{i}]", step)
      lambda_si = jnp.log(alpha_si + 1e-10) - jnp.log(sigma_si + 1e-10)
      check_nan_jit(lambda_si, f"P rk_d1 lambda_si[{i}]", step)

      rk = (lambda_si - lambda_s0) / h
      check_nan_jit(rk, f"P rk_d1 rk[{i}]", step)
      Di = (mi - m0) / rk
      check_nan_jit(Di, f"P rk_d1 Di[{i}]", step)

      rks = rks.at[i].set(rk)
      D1s = D1s.at[i].set(Di)
      return rks, D1s

    rks_init = jnp.zeros(self.config.solver_order, dtype=h.dtype)
    D1s_init = jnp.zeros((self.config.solver_order - 1, *m0.shape), dtype=m0.dtype)
    if self.config.solver_order == 1:
      # Dummy D1s array. It will not be used if order == 1
      D1s_init = jnp.zeros((1, *m0.shape), dtype=m0.dtype)
    rks, D1s = jax.lax.fori_loop(0, order - 1, rk_d1_loop_body, (rks_init, D1s_init))
    check_nan_jit(rks, "P rks after loop", step)
    check_nan_jit(D1s, "P D1s after loop", step)
    rks = rks.at[order - 1].set(1.0)
    check_nan_jit(rks, "P rks final", step)

    hh = -h if self.config.predict_x0 else h
    check_nan_jit(hh, "P hh", step)
    h_phi_1 = jnp.expm1(hh)
    check_nan_jit(h_phi_1, "P h_phi_1", step)

    if self.config.solver_type == "bh1":
      B_h = hh
    elif self.config.solver_type == "bh2":
      B_h = jnp.expm1(hh)
    else:
      raise NotImplementedError()
    check_nan_jit(B_h, "P B_h", step)

    def rb_loop_body(i, carry):
      R, b, current_h_phi_k, factorial_val = carry
      check_nan_jit(current_h_phi_k, f"P rb_loop[{i}] current_h_phi_k IN", step)
      check_nan_jit(factorial_val, f"P rb_loop[{i}] factorial_val IN", step)
      R = R.at[i].set(jnp.power(rks, i))
      b = b.at[i].set(current_h_phi_k * factorial_val / B_h)

      def update_fn(vals):
        _h_phi_k, _fac = vals
        next_fac = _fac * (i + 2)
        check_nan_jit(next_fac, f"P rb_loop[{i}] next_fac", step)
        next_h_phi_k = _h_phi_k / hh - 1.0 / next_fac
        check_nan_jit(next_h_phi_k, f"P rb_loop[{i}] next_h_phi_k", step)
        return next_h_phi_k, next_fac

      current_h_phi_k, factorial_val = jax.lax.cond(
          i < order - 1,
          update_fn,
          lambda vals: vals,
          (current_h_phi_k, factorial_val),
      )
      return R, b, current_h_phi_k, factorial_val

    R_init = jnp.zeros((self.config.solver_order, self.config.solver_order), dtype=h.dtype)
    b_init = jnp.zeros(self.config.solver_order, dtype=h.dtype)
    init_h_phi_k = h_phi_1 / hh - 1.0
    check_nan_jit(init_h_phi_k, "P init_h_phi_k", step)
    init_factorial = 1.0
    R, b, _, _ = jax.lax.fori_loop(0, order, rb_loop_body, (R_init, b_init, init_h_phi_k, init_factorial))
    check_nan_jit(R, "P R after loop", step)
    check_nan_jit(b, "P b after loop", step)


    if len(D1s) > 0:
      D1s = jnp.stack(D1s, axis=1)  # Resulting shape (B, K, C, H, W)
    check_nan_jit(D1s, "P D1s_stacked", step)

    def solve_for_rhos_p(R_mat, b_vec, current_order):
      # Create a mask for the top-left (current_order - 1) x (current_order - 1) sub-matrix
      mask_size = self.config.solver_order - 1
      mask = jnp.arange(mask_size) < (current_order - 1)
      mask_2d = mask[:, None] & mask[None, :]

      # Pad R with identity and b with zeros for a safe solve
      R_safe = jnp.where(
          mask_2d,
          R_mat[:mask_size, :mask_size],
          jnp.eye(mask_size, dtype=R_mat.dtype),
      )
      b_safe = jnp.where(mask, b_vec[:mask_size], 0.0)
      check_nan_jit(R_safe, "P solve R_safe", step)
      check_nan_jit(b_safe, "P solve b_safe", step)

      # Solve the system and mask the result
      solved_rhos = jnp.linalg.solve(R_safe, b_safe)
      check_nan_jit(solved_rhos, "P solve solved_rhos", step)
      return jnp.where(mask, solved_rhos, 0.0)

    # Handle the special case for order == 2
    if self.config.solver_order == 1:
      # Dummy rhos_p_padded for tracing.
      rhos_p_order2 = jnp.zeros(1, dtype=x.dtype)
    else:
      rhos_p_order2 = jnp.zeros(self.config.solver_order - 1, dtype=x.dtype).at[0].set(0.5)

    # Get the result for the general case
    rhos_p_general = solve_for_rhos_p(R, b, order)
    check_nan_jit(rhos_p_general, "P rhos_p_general", step)

    # Select the appropriate result based on the order
    rhos_p = jnp.where(order == 2, rhos_p_order2, rhos_p_general)
    check_nan_jit(rhos_p, "P rhos_p", step)

    pred_res = jax.lax.cond(
        order > 1,
        lambda _: jnp.einsum("k,bkc...->bc...", rhos_p, D1s).astype(x.dtype),
        # False branch: return a zero tensor with the correct shape.
        lambda _: jnp.zeros_like(x),
        operand=None,
    )
    check_nan_jit(pred_res, "P pred_res", step)

    if self.config.predict_x0:
      x_t_ = sigma_t / (sigma_s0) * x - alpha_t * h_phi_1 * m0
      check_nan_jit(x_t_, "P x_t_ term", step)
      term2 = alpha_t * B_h * pred_res
      check_nan_jit(term2, "P term2", step)
      x_t = x_t_ - term2
    else:  # Predict epsilon
      x_t_ = alpha_t / (alpha_s0) * x - sigma_t * h_phi_1 * m0
      check_nan_jit(x_t_, "P x_t_ term eps", step)
      term2 = sigma_t * B_h * pred_res
      check_nan_jit(term2, "P term2 eps", step)
      x_t = x_t_ - term2
    check_nan_jit(x_t, "P final x_t", step)
    return x_t.astype(x.dtype)

  def multistep_uni_c_bh_update(
      self,
      state: UniPCMultistepSchedulerState,
      this_model_output: jnp.ndarray,
      last_sample: jnp.ndarray,  # Sample after predictor `x_{t-1}`
      this_sample: jnp.ndarray,  # Sample before corrector `x_t` (after predictor step)
      order: int,
      step: jax.Array,
  ) -> jnp.ndarray:
    """
    One step for the UniC (B(h) version) - the Corrector.
    """
    model_output_list = state.model_outputs
    m0 = model_output_list[self.config.solver_order - 1]  # Most recent model output from history

    if last_sample is not None:
      x = last_sample
    else:
      # If it's None, create dummy data. This is for the tracing purpose
      x = jnp.zeros_like(this_sample)

    x_t = this_sample

    model_t = this_model_output

    sigma_t_val = state.sigmas[state.step_index]
    sigma_s0_val = state.sigmas[state.step_index - 1]

    alpha_t, sigma_t = self._sigma_to_alpha_sigma_t(sigma_t_val)
    alpha_s0, sigma_s0 = self._sigma_to_alpha_sigma_t(sigma_s0_val)

    lambda_t = jnp.log(alpha_t + 1e-10) - jnp.log(sigma_t + 1e-10)
    lambda_s0 = jnp.log(alpha_s0 + 1e-10) - jnp.log(sigma_s0 + 1e-10)

    h = lambda_t - lambda_s0

    def rk_d1_loop_body(i, carry):
      # Loop from i = 0 to order-1.
      rks, D1s = carry

      # Get history from state buffer
      history_idx = self.config.solver_order - (i + 2)
      mi = state.model_outputs[history_idx]
      si_val = state.timestep_list[history_idx]  # This is the actual timestep value

      alpha_si, sigma_si = self._sigma_to_alpha_sigma_t(state.sigmas[self.index_for_timestep(state, si_val)])
      lambda_si = jnp.log(alpha_si + 1e-10) - jnp.log(sigma_si + 1e-10)

      rk = (lambda_si - lambda_s0) / h
      Di = (mi - m0) / rk

      # Update pre-allocated arrays
      rks = rks.at[i].set(rk)
      D1s = D1s.at[i].set(Di)
      return rks, D1s

    # Pre-allocate arrays to max possible size
    rks_init = jnp.zeros(self.config.solver_order, dtype=h.dtype)
    D1s_init = jnp.zeros((self.config.solver_order - 1, *m0.shape), dtype=m0.dtype)
    if self.config.solver_order == 1:
      # Dummy D1s array. It will not be used if order == 1. This is for tracing.
      D1s_init = jnp.zeros((1, *m0.shape), dtype=m0.dtype)

    # Run the loop up to `order - 1`
    rks, D1s = jax.lax.fori_loop(0, order - 1, rk_d1_loop_body, (rks_init, D1s_init))

    rks = rks.at[order - 1].set(1.0)

    hh = -h if self.config.predict_x0 else h
    h_phi_1 = jnp.expm1(hh)

    if self.config.solver_type == "bh1":
      B_h = hh
    elif self.config.solver_type == "bh2":
      B_h = jnp.expm1(hh)
    else:
      raise NotImplementedError()

    def rb_loop_body(i, carry):
      # Loop from i = 0 to order-1
      R, b, current_h_phi_k, factorial_val = carry

      R = R.at[i].set(jnp.power(rks, i))
      b = b.at[i].set(current_h_phi_k * factorial_val / B_h)

      # Conditionally update phi_k and factorial for the next iteration
      def update_fn(vals):
        # This branch is taken if i < order - 1
        _h_phi_k, _fac = vals
        next_fac = _fac * (i + 2)
        next_h_phi_k = _h_phi_k / hh - 1.0 / next_fac
        return next_h_phi_k, next_fac

      current_h_phi_k, factorial_val = jax.lax.cond(
          i < order - 1,
          update_fn,  # If true, update values
          lambda vals: vals,  # If false, pass through
          (current_h_phi_k, factorial_val),
      )
      return R, b, current_h_phi_k, factorial_val

    # Pre-allocate R and b to max size
    R_init = jnp.zeros((self.config.solver_order, self.config.solver_order), dtype=h.dtype)
    b_init = jnp.zeros(self.config.solver_order, dtype=h.dtype)

    # Initialize loop carriers
    init_h_phi_k = h_phi_1 / hh - 1.0
    init_factorial = 1.0

    R, b, _, _ = jax.lax.fori_loop(0, order, rb_loop_body, (R_init, b_init, init_h_phi_k, init_factorial))

    if len(D1s) > 0:
      D1s = jnp.stack(D1s, axis=1)  # (B, K, C, H, W)

    def solve_for_rhos(R_mat, b_vec, current_order):
      # Create a mask to select the first `current_order` elements
      mask = jnp.arange(self.config.solver_order) < current_order
      mask_2d = mask[:, None] & mask[None, :]

      # Pad R with identity and b with zeros to create a safe, full-sized system
      R_safe = jnp.where(mask_2d, R_mat, jnp.eye(self.config.solver_order, dtype=R_mat.dtype))
      b_safe = jnp.where(mask, b_vec, 0.0)

      # Solve the full-size system and mask the result
      solved_rhos = jnp.linalg.solve(R_safe, b_safe)
      return jnp.where(mask, solved_rhos, 0.0)

    rhos_c_order1 = jnp.zeros(self.config.solver_order, dtype=x_t.dtype).at[0].set(0.5)
    rhos_c_general = solve_for_rhos(R, b, order)
    rhos_c = jnp.where(order == 1, rhos_c_order1, rhos_c_general)

    D1_t = model_t - m0

    corr_res = jax.lax.cond(
        order > 1,
        lambda _: (jnp.einsum("k,bkc...->bc...", rhos_c[:-1], D1s)),
        lambda _: jnp.zeros_like(D1_t),
        operand=None,
    )

    final_rho = jnp.dot(
        rhos_c,
        jax.nn.one_hot(order - 1, self.config.solver_order, dtype=rhos_c.dtype),
    )

    if self.config.predict_x0:
      x_t_ = sigma_t / sigma_s0 * x - alpha_t * h_phi_1 * m0
      x_t = x_t_ - alpha_t * B_h * (corr_res + final_rho * D1_t)
    else:
      x_t_ = alpha_t / alpha_s0 * x - sigma_t * h_phi_1 * m0
      x_t = x_t_ - sigma_t * B_h * (corr_res + final_rho * D1_t)
    
    check_nan_jit(x_t, "corrector x_t", step)
    return x_t.astype(x.dtype)

  def index_for_timestep(
      self,
      state: UniPCMultistepSchedulerState,
      timestep: Union[int, jnp.ndarray],
      schedule_timesteps: Optional[jnp.ndarray] = None,
  ) -> int:
    """ "Gets the step_index for timestep."""
    if schedule_timesteps is None:
      schedule_timesteps = state.timesteps

    # QUINN!!
    # timestep_val = (
    #     timestep.item()
    #     if isinstance(timestep, jnp.ndarray) and timestep.ndim == 0
    #     else timestep
    # )
    timestep_val = timestep

    index_candidates = jnp.where(schedule_timesteps == timestep_val, size=1, fill_value=-1)[0]

    step_index = jnp.where(
        index_candidates[0] == -1,  # No match found
        len(schedule_timesteps) - 1,  # Default to last index
        index_candidates[0],
    )
    return step_index

  def _init_step_index(
      self, state: UniPCMultistepSchedulerState, timestep: Union[int, jnp.ndarray]
  ) -> UniPCMultistepSchedulerState:
    """Initializes the step_index counter for the scheduler."""
    if state.begin_index is None:
      step_index_val = self.index_for_timestep(state, timestep)
      return state.replace(step_index=step_index_val)
    else:
      return state.replace(step_index=state.begin_index)

  @partial(jax.jit, static_argnums=(0, 5))  # self is static_argnum=0
  def step(
      self,
      state: UniPCMultistepSchedulerState,
      model_output: jnp.ndarray,  # This is the direct output from the diffusion model (e.g., noise prediction)
      timestep: Union[int, jnp.ndarray],  # Current discrete timestep from the scheduler's sequence
      sample: jnp.ndarray,  # Current noisy sample (latent)
      return_dict: bool = True,
      generator: Optional[jax.random.PRNGKey] = None,  # JAX random key
  ) -> Union[FlaxUniPCMultistepSchedulerOutput, Tuple[jnp.ndarray]]:
    """
    Predict the sample from the previous timestep by reversing the SDE. This function propagates the sample with
    the multistep UniPC.
    """
    step_val = state.step_index # For debug, might be None initially

    check_nan_jit(model_output, "step input model_output", step_val)
    check_nan_jit(sample, "step input sample", step_val)

    sample = sample.astype(jnp.float32)

    if state.timesteps is None:
      raise ValueError("Number of inference steps is 'None', you need to run 'set_timesteps' after creating the scheduler")

    timestep_scalar = jnp.array(timestep)

    # Initialize step_index if it's the first step
    if state.step_index is None:
      state = self._init_step_index(state, timestep_scalar)
    step_val = state.step_index 

    # Determine if corrector should be used
    use_corrector = (
        (state.step_index > 0)
        & (~jnp.isin(state.step_index - 1, jnp.array(self.config.disable_corrector)))
        & (state.last_sample is not None)
    )

    # Convert model_output (noise/v_pred) to x0_pred or epsilon_pred, based on prediction_type
    model_output_for_history = self.convert_model_output(state, model_output, sample, step_val)
    check_nan_jit(model_output_for_history, "model_output_for_history", step_val)

    # Apply corrector if applicable
    sample = jax.lax.cond(
        use_corrector,
        lambda: self.multistep_uni_c_bh_update(
            state=state,
            this_model_output=model_output_for_history,
            last_sample=state.last_sample,
            this_sample=sample,
            order=state.this_order,
            step=step_val
        ),
        lambda: sample,
    )
    check_nan_jit(sample, "sample_corrected", step_val)

    # Update history buffers (model_outputs and timestep_list)
    # Shift existing elements to the left and add new one at the end.
    # `state.model_outputs` and `state.timestep_list` are fixed-size arrays.
    # Example:
    # t0:[None,...,model_output0]
    # t1:[None,..model_output0,model_output1]
    # ...
    # tn:[model_output0,model_output1,...,model_output_n]
    def step_idx0_branch():
      updated_model_outputs_history = state.model_outputs.at[-1].set(model_output_for_history)
      updated_timestep_list_history = state.timestep_list.at[-1].set(timestep_scalar)
      return updated_model_outputs_history, updated_timestep_list_history

    def non_step_idx0_branch():
      updated_model_outputs_history = jnp.roll(state.model_outputs, shift=-1, axis=0)
      updated_model_outputs_history = updated_model_outputs_history.at[-1].set(model_output_for_history)

      updated_timestep_list_history = jnp.roll(state.timestep_list, shift=-1)
      updated_timestep_list_history = updated_timestep_list_history.at[-1].set(timestep_scalar)
      return updated_model_outputs_history, updated_timestep_list_history

    updated_model_outputs_history, updated_timestep_list_history = jax.lax.cond(
        state.step_index == 0, step_idx0_branch, non_step_idx0_branch
    )
    state = state.replace(
        model_outputs=updated_model_outputs_history,
        timestep_list=updated_timestep_list_history,
    )

    # Determine the order for the current step (warmup phase logic)
    this_order = jnp.where(
        self.config.lower_order_final,
        jnp.minimum(self.config.solver_order, len(state.timesteps) - state.step_index),
        self.config.solver_order,
    )

    # Warmup for multistep: `this_order` can't exceed `lower_order_nums + 1`
    new_this_order = jnp.minimum(this_order, state.lower_order_nums + 1)
    state = state.replace(this_order=new_this_order)

    # Store current sample as `last_sample` for the *next* step's corrector
    state = state.replace(last_sample=sample)

    # UniP predictor step
    prev_sample = self.multistep_uni_p_bh_update(
        state=state,
        model_output=model_output,
        sample=sample,
        order=state.this_order,
        step=step_val,
    )

    # Update lower_order_nums for warmup
    new_lower_order_nums = jnp.where(
        state.lower_order_nums < self.config.solver_order,
        state.lower_order_nums + 1,
        state.lower_order_nums,
    )
    state = state.replace(lower_order_nums=new_lower_order_nums)
    # Upon completion, increase step index by one
    state = state.replace(step_index=state.step_index + 1)

    # Return the updated sample and state
    if not return_dict:
      return (prev_sample, state)

    return FlaxUniPCMultistepSchedulerOutput(prev_sample=prev_sample, state=state)

  def scale_model_input(self, state: UniPCMultistepSchedulerState, sample: jnp.ndarray, *args, **kwargs) -> jnp.ndarray:
    """
    UniPC does not scale model input, so it returns the sample unchanged.
    """
    return sample

  def add_noise(
      self,
      state: UniPCMultistepSchedulerState,
      original_samples: jnp.ndarray,
      noise: jnp.ndarray,
      timesteps: jnp.ndarray,
  ) -> jnp.ndarray:
    return add_noise_common(state.common, original_samples, noise, timesteps)

  def _sigma_to_alpha_sigma_t(self, sigma):
    if self.config.use_flow_sigmas:
      alpha_t = 1 - sigma
      sigma_t = sigma
    else:
      alpha_t = 1 / ((sigma**2 + 1) ** 0.5)
      sigma_t = sigma * alpha_t

    return alpha_t, sigma_t

  def __len__(self) -> int:
    return self.config.num_train_timesteps
